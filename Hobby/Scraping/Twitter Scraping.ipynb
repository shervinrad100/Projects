{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copied from:\n",
    "\n",
    "https://github.com/ZacLanghorne/FBSentimentAnalysis/blob/master/FacebookSentimentAnalysis.ipynb\n",
    "\n",
    "steps:\n",
    "\n",
    "1. _tokenising_\n",
    "\n",
    "transforms the full sentences into words by removing stop words, conjunctions, etc\n",
    "\n",
    "2. _normalising and noise reduction_\n",
    "\n",
    "this is done by lemmatising the tweets; meaning it takes the words to their root. For example, you have are, am, is --> to be or cars, car's, car --> car. \n",
    "\n",
    "- determine word density \n",
    "- build model\n",
    "- explorative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "from nltk.corpus import twitter_samples # pre scraped tweets\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = stopwords.words('english') # enlgish tokenisation\n",
    "from nltk import FreqDist # stats \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "# remove noise\n",
    "import re\n",
    "import string\n",
    "# \n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sherv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for tokenisation\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sherv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sherv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sherv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toeknising \n",
    "nltk.download('punkt')\n",
    "# normalising and lemmatising text\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\sherv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get twitter samples \n",
    "# scraping twitter is only allowed under certain conditions\n",
    "# we train with sample data and apply to scraped data\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is twitter samples\n",
    "*fileids*\n",
    "- to know what the strings method takes\n",
    "\n",
    "*strings*\n",
    "- gets tweets in json format\n",
    "\n",
    "*tokenized*\n",
    "- you get tokenised tweets but we want to tokenise it ourselves to practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
    "negative_tweets = twitter_samples.strings(\"negative_tweets.json\")\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise the tweets\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalising and removing noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is re (regular expression operations)\n",
    "\n",
    "1. A regular expression (or RE) specifies a set of strings that matches it. The .sub() method substitutes it with the given string.\n",
    "\n",
    "2. re contains special characters as well so if we want to search for certain characters we need to escape them with `\\`. These characters are:\n",
    "    - `.` - searches for any character except new line\n",
    "    - `\\d` - digits (0-9)\n",
    "    - `\\D` - not a digit (0-9)\n",
    "    - `w` - word character (a-z, A-Z, 0-9, _)\n",
    "    - `W` - not a word character\n",
    "    - `s` - white space\n",
    "    - `S` - not a white space \n",
    "    <br>\n",
    "    <br>\n",
    "    Anchors (invisible positions before or after characters):\n",
    "    - `b` - word boundary (\"\\bHa\", \"Ha HaHa\") would return \"**Ha** **Ha**Ha\"\n",
    "    - `B` - Not a word boundary (\"\\bHa\", \"Ha HaHa\") would return \"Ha Ha**Ha**\"\n",
    "    - `^` - Beginning of a string (\"^http\", \"http://www.blabla/http\") would return \"**http**://www.blabla/http\"\n",
    "    - `DollarSign` - end of string (\"http`DollarSign`\", \"http://www.blabla/http\") would return \"http://www.blabla/**http**\"\n",
    "    <br>\n",
    "    <br>\n",
    "    - `[]` - searches for whatever is in the bracket eg: (\"\\d[-.]\\d\", \"1.1, 2-3, 2/2\") would return \"**1.1**, **2-3**, 2/2\" <br>\n",
    "    &nbsp; `[1-5]` creates a range and will choose ONE character in that range and list <br>\n",
    "    &nbsp; `[^a-z]` using `^` within a character set searches for everything except what's included in the set eg (\"[^b]at\",  \"cat, pat, bat\") returns \"**cat**, **pat**, bat\"\n",
    "    - `|` - either or\n",
    "    - `()` - group\n",
    "    <br>\n",
    "    <br>\n",
    "    Quantifiers:\n",
    "    - `*` - 0 or more\n",
    "    - `+` - 1 or more\n",
    "    - `?` - 0 or one\n",
    "    - `{3}` - exact number eg: (\"\\d{3}\", \"123, 1234, 12, 1\") will return  \"**123**, 1234, 12, 1\"\n",
    "    - `{3,4}` - range of numbers\n",
    "    \n",
    "When you create groups, you can select them from the returned object by the re. Group 0 is always the entire match returned. eg: <br>\n",
    "```python\n",
    "url = \"https://www.google.com, https://blabla.com\"\n",
    "pattern = re.compile(r\"https?://(www\\.)?(\\w+)(\\.\\w+)\") # group 1: www. group 2: subdomain group 3:.domain\n",
    "matches = pattern.finditer(url) # or use pattern.findall(url)\n",
    "for match in matches:\n",
    "    print(match.group(1))\n",
    "```\n",
    "\n",
    "you can also refer to these groups when substituting. eg: <br>\n",
    "```python\n",
    "subbed_url = pattern.sub(r\"\\2\\3\", url) # subs groups 2 and 3\n",
    "```\n",
    "\n",
    "Finally, you can ignore case using a flag. eg <br>\n",
    "```python\n",
    "pattern = re.compile(r\"start\", re.IGNORECASE) # shorthand flag is re.I\n",
    "matches = pattern.findall(url)\n",
    "print(matches)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub(r\"^https?:\\/\\/.*[\\r\\n]*\",\"\", token) # removes https://<0 or more characters><0 or more new lines>\n",
    "        token = re.sub(r\"(@[A-Za-z0-9_]+)\",\"\", token) # removes mentions from the tweet\n",
    "        \n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = \"n\" # assign names as nouns\n",
    "        elif tag.startswith(\"VB\"):\n",
    "            pos = \"v\" # assign verbs as verbs\n",
    "        else:\n",
    "            pos = \"a\"\n",
    "            \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token,pos) # reduce words to their root words\n",
    "        \n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            # gets rid of the empty tweets and any punctuation\n",
    "            cleaned_tokens.append(token.lower())\n",
    "        \n",
    "        return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words)) \n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an iterator for all the words in the dictionary\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "all_neg_words = get_all_words(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a frequency distribution to find most common words.\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "freq_dist_neg = FreqDist(all_neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the values to a dictionary for us in naive bayes classification.\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the lists of model ready data.\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in to train and test data.\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model] # Label the positive data.\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model] # Label the negative data.\n",
    "\n",
    "dataset = positive_dataset + negative_dataset # Combine the data sets.\n",
    "\n",
    "random.shuffle(dataset) # Shuffle the data so theres no natural ordering.\n",
    "\n",
    "train_data = dataset[:int(len(dataset)*0.7)] # First 7000 entries for train.\n",
    "test_data = dataset[int(len(dataset)*0.7):] # Final 3000 for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A ELE probability distribution must have at least one bin.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-1875c30bd894>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\naivebayes.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;31m# Create the P(label) distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[0mlabel_probdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_freqdist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[1;31m# Create the P(fval|label, fname) distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, freqdist, bins)\u001b[0m\n\u001b[0;32m    935\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspecified\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mdefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfreqdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m         \"\"\"\n\u001b[1;32m--> 937\u001b[1;33m         \u001b[0mLidstoneProbDist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreqdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, freqdist, gamma, bins)\u001b[0m\n\u001b[0;32m    810\u001b[0m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m             raise ValueError(\n\u001b[1;32m--> 812\u001b[1;33m                 \u001b[1;34m'A %s probability distribution '\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'must have at least one bin.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    813\u001b[0m             )\n\u001b[0;32m    814\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mfreqdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: A ELE probability distribution must have at least one bin."
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_data) # must creaate dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the accuracy and the words that are most useful in determing the sentiment.\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the model with custom tweets!\n",
    "custom_tweet = \"Test message\"\n",
    "custom_tokens = remove_noise(nltk.word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape",
   "language": "python",
   "name": "scrape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
