{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copied from:\n",
    "\n",
    "https://github.com/ZacLanghorne/FBSentimentAnalysis/blob/master/FacebookSentimentAnalysis.ipynb\n",
    "\n",
    "steps:\n",
    "\n",
    "1. _tokenising_\n",
    "\n",
    "transforms the full sentences into words by removing stop words, conjunctions, etc\n",
    "\n",
    "2. _normalising and noise reduction_\n",
    "\n",
    "this is done by lemmatising the tweets; meaning it takes the words to their root. For example, you have are, am, is --> to be or cars, car's, car --> car. \n",
    "\n",
    "- determine word density \n",
    "- build model\n",
    "- explorative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sherv\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "from nltk.corpus import twitter_samples # pre scraped tweets\n",
    "stop_words = stopwords.words('english') # enlgish tokenisation\n",
    "from nltk import FreqDist # stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sherv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for tokenisation\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sherv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sherv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sherv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toeknising \n",
    "nltk.download('punkt')\n",
    "# normalising and lemmatising text\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\sherv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get twitter samples \n",
    "# scraping twitter is only allowed under certain conditions\n",
    "# we train with sample data and apply to scraped data\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove noise\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is twitter samples\n",
    "*fileids*\n",
    "- to know what the strings method takes\n",
    "\n",
    "*strings*\n",
    "- gets tweets in json format\n",
    "\n",
    "*tokenized*\n",
    "- you get tokenised tweets but we want to tokenise it ourselves to practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
    "negative_tweets = twitter_samples.strings(\"negative_tweets.json\")\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise the tweets\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalising and removing noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is re (regular expression operations)\n",
    "\n",
    "1. A regular expression (or RE) specifies a set of strings that matches it. The .sub() method substitutes it with the given string.\n",
    "\n",
    "2. re contains special characters as well so if we want to search for certain characters we need to escape them with `\\`. These characters are:\n",
    "    - `.` - searches for any character except new line\n",
    "    - `\\d` - digits (0-9)\n",
    "    - `\\D` - not a digit (0-9)\n",
    "    - `w` - word character (a-z, A-Z, 0-9, _)\n",
    "    - `W` - not a word character\n",
    "    - `s` - white space\n",
    "    - `S` - not a white space \n",
    "    <br>\n",
    "    <br>\n",
    "    Anchors (invisible positions before or after characters):\n",
    "    - `b` - word boundary (\"\\bHa\", \"Ha HaHa\") would return \"**Ha** **Ha**Ha\"\n",
    "    - `B` - Not a word boundary (\"\\bHa\", \"Ha HaHa\") would return \"Ha Ha**Ha**\"\n",
    "    - `^` - Beginning of a string (\"^http\", \"http://www.blabla/http\") would return \"**http**://www.blabla/http\"\n",
    "    - `DollarSign` - end of string (\"http`DollarSign`\", \"http://www.blabla/http\") would return \"http://www.blabla/**http**\"\n",
    "    <br>\n",
    "    <br>\n",
    "    - `[]` - searches for whatever is in the bracket eg: (\"\\d[-.]\\d\", \"1.1, 2-3, 2/2\") would return \"**1.1**, **2-3**, 2/2\" <br>\n",
    "    &nbsp; `[1-5]` creates a range and will choose ONE character in that range and list <br>\n",
    "    &nbsp; `[^a-z]` using `^` within a character set searches for everything except what's included in the set eg (\"[^b]at\",  \"cat, pat, bat\") returns \"**cat**, **pat**, bat\"\n",
    "    - `|` - either or\n",
    "    - `()` - group\n",
    "    <br>\n",
    "    <br>\n",
    "    Quantifiers:\n",
    "    - `*` - 0 or more\n",
    "    - `+` - 1 or more\n",
    "    - `?` - 0 or one\n",
    "    - `{3}` - exact number eg: (\"\\d{3}\", \"123, 1234, 12, 1\") will return  \"**123**, 1234, 12, 1\"\n",
    "    - `{3,4}` - range of numbers\n",
    "    \n",
    "When you create groups, you can select them from the returned object by the re. Group 0 is always the entire match returned. eg: <br>\n",
    "```python\n",
    "url = \"https://www.google.com, https://blabla.com\"\n",
    "pattern = re.compile(r\"https?://(www\\.)?(\\w+)(\\.\\w+)\") # group 1: www. group 2: subdomain group 3:.domain\n",
    "matches = pattern.finditer(url) # or use pattern.findall(url)\n",
    "for match in matches:\n",
    "    print(match.group(1))\n",
    "```\n",
    "\n",
    "you can also refer to these groups when substituting. eg: <br>\n",
    "```python\n",
    "subbed_url = pattern.sub(r\"\\2\\3\", url) # subs groups 2 and 3\n",
    "```\n",
    "\n",
    "Finally, you can ignore case using a flag. eg <br>\n",
    "```python\n",
    "pattern = re.compile(r\"start\", re.IGNORECASE) # shorthand flag is re.I\n",
    "matches = pattern.findall(url)\n",
    "print(matches)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "    cleaned = []\n",
    "    \n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub(r\"^https?:\\/\\/.*[\\r\\n]*\",\"\", token) # removes https://<0 or more characters><0 or more new lines>\n",
    "        token = re.sub(r\"(@[A-Za-z0-9_]+)\",\"\", token) # removes mentions from the tweet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape",
   "language": "python",
   "name": "scrape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
